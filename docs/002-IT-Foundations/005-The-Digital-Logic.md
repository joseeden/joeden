---
title: "Digital Logic"
description: "Binary, ASCII, and the sorts"
sidebar_position: 5
tags: [IT Fundamentals, Computers, Operating Systems]
last_update:
  date: 2/26/2017
---



## Computations

Let's delve into the fundamental workings of computers through a simple math problem: 

```
0 + 1 equals what?
```

It's a quick answer of 1, but imagine needing to perform 1,000 or even 1 billion such calculations. This is precisely what computers excel at.

- Computers process information using the binary system, a base-2 numeral system.
- Involves only two digits: 1s and 0s, which represent data in electronic form.
- Each binary digit, or bit, forms the basic unit of information storage and processing in computers.

Understanding the binary system is crucial because it underpins all digital computing operations, allowing us to communicate and manipulate data electronically.

## Binary System

In computing, the binary system is the foundation of all digital operations, representing data and instructions using only two digits: 1 and 0. Think of it like our alphabet, where combinations of letters form words and sentences.

- Binary groups data into 8-bit units called bytes.
- Bytes are capable of representing up to 256 different values.
- Originally, 8 bits became standard due to the practical limitations.

Understanding the binary system's historical and practical implications helps us grasp how computers process and interpret information today.

[](/img/docs/binary.jpg)


## Take a Byte

In computing, a byte consists of 8 bits and serves as the basic unit of data storage and processing.

- A byte can represent a single character, number, or symbol in digital communication.
- Allows for more complex digital information, such as text and multimedia content.
- Relies on bytes to manage and store vast amounts of information efficiently.

Bytes form the building blocks of digital communication and enable the creation of rich and diverse content in the digital world.

[](/img/docs/bit-bytes.png)


## Character Encoding

Character encoding bridges the gap between binary data and human-readable text, making digital communication accessible and meaningful.

- Translates binary data into recognizable characters like letters, numbers, and symbols.
- Enables computers to display text and multimedia content in various languages and formats.

ASCII (American Standard Code for Information Interchange) was the first widely used character encoding standard, encompassing basic English characters and symbols.

## ASCII

ASCII (American Standard Code for Information Interchange) was the initial character encoding standard, representing English characters and symbols using 7-bit binary codes. It provided a standardized way to encode text and control characters in early computing systems.

[](/img/docs/ASCII-Table.png)

ASCII established a foundation for digital communication and data interchange in computer networks and systems.

## UTF-8

UTF-8 (Unicode Transformation Format, 8-bit) is a versatile character encoding standard that extends ASCII to support multiple languages and symbols.

- Compatibile with ASCII, while accommodating a broader range of characters and symbols.
- Represents diverse linguistic and cultural content in digital environments.

UTF-8's flexibility and efficiency make it the predominant character encoding standard for web pages, software applications, and digital communications worldwide.

## RGB

RGB (Red, Green, Blue) is a color model used in computing to represent and display colors on screens and digital devices. RGB combines varying intensities of red, green, and blue light to create a wide spectrum of colors. Each RGB value specifies the intensity of its respective color component, allowing for precise color representation on screens. This enables developers and designers to create visually appealing content and applications across digital platforms.

[](/img/docs/rgb-wheel.png)
