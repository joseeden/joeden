{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/joseeden/joeden/blob/master/docs/021-Software-Engineering/021-Jupyter-Notebooks/001-Sample-Notebooks/004-aggregating_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walmart Dataset Overview  \n",
    "\n",
    "The dataset contains weekly sales data for Walmart stores, including:  \n",
    "\n",
    "- Store ID and type  \n",
    "- Weekly sales and department IDs  \n",
    "- Holiday week indicator  \n",
    "- Average temperature and fuel prices  \n",
    "- National unemployment rate  \n",
    "\n",
    "The dataset can be downloaded here: [sales_subset.csv](@site/assets/datasets/pandas-aggregating-data/sales_subset.csv)\n",
    "\n",
    "Sample rows:  \n",
    "\n",
    "| Store | Sales    | Dept | Holiday | Temp (Â°C) | Fuel Price (USD/L) | Unemployment |\n",
    "|-------|----------|------|---------|-----------|---------------------|--------------|\n",
    "|   1   | 24924.50 |   1  |    0    |   5.73    |         0.68        |     8.11     |\n",
    "|   1   | 21827.90 |   1  |    0    |   8.06    |         0.69        |     8.11     |\n",
    "|   1   | 57258.43 |   1  |    0    |  16.82    |         0.72        |     7.81     |\n",
    "|   1   | 17413.94 |   1  |    0    |  22.53    |         0.75        |     7.81     |\n",
    "|   1   | 17558.09 |   1  |    0    |  27.05    |         0.71        |     7.81     |\n",
    "|   1   | 16333.14 |   1  |    0    |  27.17    |         0.71        |     7.79     | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean and Median\n",
    "\n",
    "Print information about the columns in sales using `head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0           0      1    A           1  2010-02-05      24924.50       False   \n",
      "1           1      1    A           1  2010-03-05      21827.90       False   \n",
      "2           2      1    A           1  2010-04-02      57258.43       False   \n",
      "3           3      1    A           1  2010-05-07      17413.94       False   \n",
      "4           4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0       5.727778              0.679451         8.106  \n",
      "1       8.055556              0.693452         8.106  \n",
      "2      16.816667              0.718284         7.808  \n",
      "3      22.527778              0.748928         7.808  \n",
      "4      27.050000              0.714586         7.808  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use URL , so that notebook can be ran from Google Colab without downloading csv file\n",
    "url = 'https://raw.githubusercontent.com/joseeden/joeden/refs/heads/master/docs/021-Software-Engineering/021-Jupyter-Notebooks/000-Sample-Datasets/data-manipulation-using-pandas/sales_subset.csv'\n",
    "sales = pd.read_csv(url)\n",
    "\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print information about the columns in sales using `info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10774 entries, 0 to 10773\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Unnamed: 0            10774 non-null  int64  \n",
      " 1   store                 10774 non-null  int64  \n",
      " 2   type                  10774 non-null  object \n",
      " 3   department            10774 non-null  int64  \n",
      " 4   date                  10774 non-null  object \n",
      " 5   weekly_sales          10774 non-null  float64\n",
      " 6   is_holiday            10774 non-null  bool   \n",
      " 7   temperature_c         10774 non-null  float64\n",
      " 8   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 9   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(3), object(2)\n",
      "memory usage: 768.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sales.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the mean of the `weekly_sales` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23843.95014850566\n"
     ]
    }
   ],
   "source": [
    "print(sales[\"weekly_sales\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the median of the `weekly_sales` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "print(sales[\"weekly_sales\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates \n",
    "\n",
    "Print the maximum of the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-02-05\n"
     ]
    }
   ],
   "source": [
    "print(sales[\"date\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the minimum of the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-02-05\n"
     ]
    }
   ],
   "source": [
    "print(sales[\"date\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Summaries  \n",
    "\n",
    "Sometimes, you need custom functions to summarize your data beyond what pandas and NumPy offer.  \n",
    "The `.agg()` method allows custom functions to a DataFrame, even across multiple columns, for efficient aggregation.  \n",
    "\n",
    "Example: \n",
    "\n",
    "```python\n",
    "df['column'].agg(function)\n",
    "```  \n",
    "\n",
    "In this exercise, the custom function calculates the \"IQR\" (inter-quartile range), which is the difference between the 75th and 25th percentiles. Using this function, print the IQR of the `temperature_c` column of sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.583333333333336\n"
     ]
    }
   ],
   "source": [
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "print(sales[\"temperature_c\"].agg(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the column selection to use the custom `iqr` function with `.agg()` to print the IQR of these in order:\n",
    "\n",
    "- `temperature_c`\n",
    "- `fuel_price_usd_per_l`\n",
    "- `unemployment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_c           16.583333\n",
      "fuel_price_usd_per_l     0.073176\n",
      "unemployment             0.565000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "print(sales[[\n",
    "    \"temperature_c\", \n",
    "    \"fuel_price_usd_per_l\",\n",
    "    \"unemployment\"\n",
    "]].agg(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the aggregation functions called by `.agg()` in order:\n",
    "\n",
    "- `iqr`\n",
    "- `np.median`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        temperature_c  fuel_price_usd_per_l  unemployment\n",
      "iqr         16.583333              0.073176         0.565\n",
      "median      16.966667              0.743381         8.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\1977351458.py:9: FutureWarning: The provided callable <function median at 0x000001C3BC841800> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  ]].agg([iqr, np.median]))\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\1977351458.py:9: FutureWarning: The provided callable <function median at 0x000001C3BC841800> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  ]].agg([iqr, np.median]))\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\1977351458.py:9: FutureWarning: The provided callable <function median at 0x000001C3BC841800> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  ]].agg([iqr, np.median]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "print(sales[[\n",
    "    \"temperature_c\", \n",
    "    \"fuel_price_usd_per_l\",\n",
    "    \"unemployment\"\n",
    "]].agg([iqr, np.median]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Statistics  \n",
    "\n",
    "Cumulative statistics help track data over time. In this exercise, you'll calculate the cumulative sum and maximum of a department's weekly sales to find the total sales so far and the highest weekly sales so far.  \n",
    "\n",
    "Create the a DataFrame `sales_1_1` with sales data for department 1 of store 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "url = 'https://raw.githubusercontent.com/joseeden/joeden/refs/heads/master/docs/021-Software-Engineering/021-Jupyter-Notebooks/000-Sample-Datasets/data-manipulation-using-pandas/sales_subset_store_1.csv'\n",
    "sales_1_1 = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort `sales_1_1` by date. Get the cumulative sum of `weekly_sales`, then add as a new column called `cum_weekly_sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0           0      1    A           1  2010-02-05      24924.50       False   \n",
      "1           1      1    A           1  2010-03-05      21827.90       False   \n",
      "2           2      1    A           1  2010-04-02      57258.43       False   \n",
      "3           3      1    A           1  2010-05-07      17413.94       False   \n",
      "4           4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  cum_weekly_sales  \n",
      "0       5.727778              0.679451         8.106          24924.50  \n",
      "1       8.055556              0.693452         8.106          46752.40  \n",
      "2      16.816667              0.718284         7.808         104010.83  \n",
      "3      22.527778              0.748928         7.808         121424.77  \n",
      "4      27.050000              0.714586         7.808         138982.86  \n"
     ]
    }
   ],
   "source": [
    "sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n",
    "print(sales_1_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the cumulative max of `weekly_sales`, then add as a new column called `cum_max_sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0           0      1    A           1  2010-02-05      24924.50       False   \n",
      "1           1      1    A           1  2010-03-05      21827.90       False   \n",
      "2           2      1    A           1  2010-04-02      57258.43       False   \n",
      "3           3      1    A           1  2010-05-07      17413.94       False   \n",
      "4           4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  cum_weekly_sales  \\\n",
      "0       5.727778              0.679451         8.106          24924.50   \n",
      "1       8.055556              0.693452         8.106          46752.40   \n",
      "2      16.816667              0.718284         7.808         104010.83   \n",
      "3      22.527778              0.748928         7.808         121424.77   \n",
      "4      27.050000              0.714586         7.808         138982.86   \n",
      "\n",
      "   cum_max_sales  \n",
      "0       24924.50  \n",
      "1       24924.50  \n",
      "2       57258.43  \n",
      "3       57258.43  \n",
      "4       57258.43  \n"
     ]
    }
   ],
   "source": [
    "sales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\n",
    "print(sales_1_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the three new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  weekly_sales  cum_weekly_sales  cum_max_sales\n",
      "0    2010-02-05      24924.50          24924.50       24924.50\n",
      "1    2010-03-05      21827.90          46752.40       24924.50\n",
      "2    2010-04-02      57258.43         104010.83       57258.43\n",
      "3    2010-05-07      17413.94         121424.77       57258.43\n",
      "4    2010-06-04      17558.09         138982.86       57258.43\n",
      "..          ...           ...               ...            ...\n",
      "896  2011-11-25       2400.00       18826604.55      140504.41\n",
      "897  2011-12-09        435.00       18827039.55      140504.41\n",
      "898  2012-02-03        330.00       18827369.55      140504.41\n",
      "899  2012-04-06        140.00       18827509.55      140504.41\n",
      "900  2012-10-05        635.00       18828144.55      140504.41\n",
      "\n",
      "[901 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping duplicates\n",
    "\n",
    "Removing duplicates is important accurate counts because often, you don't want to count the same thing multiple times. \n",
    "\n",
    "Using the same `sales` dataframe, remove rows with duplicate pairs of `store` and `type`, and save as `store_types` and print the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  store type  department        date  weekly_sales  \\\n",
      "0              0      1    A           1  2010-02-05      24924.50   \n",
      "901          901      2    A           1  2010-02-05      35034.06   \n",
      "1798        1798      4    A           1  2010-02-05      38724.42   \n",
      "2699        2699      6    A           1  2010-02-05      25619.00   \n",
      "3593        3593     10    B           1  2010-02-05      40212.84   \n",
      "\n",
      "      is_holiday  temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0          False       5.727778              0.679451         8.106  \n",
      "901        False       4.550000              0.679451         8.324  \n",
      "1798       False       6.533333              0.686319         8.623  \n",
      "2699       False       4.683333              0.679451         7.259  \n",
      "3593       False      12.411111              0.782478         9.765  \n"
     ]
    }
   ],
   "source": [
    "store_types = sales.drop_duplicates(subset=['store','type'])\n",
    "print(store_types.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows of `sales` with duplicate pairs of `store` and `department`, and save as `store_depts` and print the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0            0      1    A           1  2010-02-05      24924.50       False   \n",
      "12          12      1    A           2  2010-02-05      50605.27       False   \n",
      "24          24      1    A           3  2010-02-05      13740.12       False   \n",
      "36          36      1    A           4  2010-02-05      39954.04       False   \n",
      "48          48      1    A           5  2010-02-05      32229.38       False   \n",
      "\n",
      "    temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0        5.727778              0.679451         8.106  \n",
      "12       5.727778              0.679451         8.106  \n",
      "24       5.727778              0.679451         8.106  \n",
      "36       5.727778              0.679451         8.106  \n",
      "48       5.727778              0.679451         8.106  \n"
     ]
    }
   ],
   "source": [
    "store_depts = sales.drop_duplicates(subset=['store','department'])\n",
    "print(store_depts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the rows that are holiday weeks using the `is_holiday` column, and drop the duplicate `dates`, saving as `holiday_dates`. Print the holiday dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498     2010-09-10\n",
      "691     2011-11-25\n",
      "2315    2010-02-12\n",
      "6735    2012-09-07\n",
      "6810    2010-12-31\n",
      "6815    2012-02-10\n",
      "6820    2011-09-09\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "holiday_dates = sales[sales['is_holiday'] == True].drop_duplicates(subset='date')\n",
    "print(holiday_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting \n",
    "\n",
    "Counting provides a quick overview of your data and helps uncover interesting patterns or anomalies you might otherwise miss. \n",
    "\n",
    "Using the `store_types` and `store_depts` dataframe from the previous example, count the number of stores of each store `type` in `store_types`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    11\n",
      "B     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the proportion of stores of each store `type` in `store_types`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    0.916667\n",
      "B    0.083333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "store_props = store_types[\"type\"].value_counts(sort=True,normalize=True)\n",
    "print(store_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of stores of each `department` in `store_depts`, sorting the counts in **descending** order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department\n",
      "1     12\n",
      "2     12\n",
      "3     12\n",
      "4     12\n",
      "5     12\n",
      "      ..\n",
      "37    10\n",
      "48     8\n",
      "50     6\n",
      "39     4\n",
      "43     2\n",
      "Name: count, Length: 80, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dept_counts_sorted = store_depts[\"department\"].value_counts()\n",
    "print(dept_counts_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the proportion of stores of each `department` in `store_depts`, sorting the proportions in **descending** order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department\n",
      "1     0.012917\n",
      "2     0.012917\n",
      "3     0.012917\n",
      "4     0.012917\n",
      "5     0.012917\n",
      "        ...   \n",
      "37    0.010764\n",
      "48    0.008611\n",
      "50    0.006459\n",
      "39    0.004306\n",
      "43    0.002153\n",
      "Name: proportion, Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True,normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Statistics\n",
    "\n",
    "Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this step, you need to calculate the total sales made at each store type. The numbers you get can then used to see what proportion of Walmart's total sales were made at each type.\n",
    "\n",
    "Calculate the total `weekly_sales` over the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256894718.89999998\n"
     ]
    }
   ],
   "source": [
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "print(sales_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset for type \"A\" stores, and calculate their total weekly sales.\n",
    "Do the same for type \"B\" and type \"C\" stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the A/B/C results into a list, and divide by sales_all to get the proportion of sales by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9097747 0.0902253 0.       ]\n"
     ]
    }
   ],
   "source": [
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing statistics computation  for a few groups is straightforward by running the same command for each group. However, this becomes tedious as when you need to run the same computation over larger datasets. The `.groupby()` method simplifies this process by performing calculations across all groups efficiently.\n",
    "\n",
    "Group sales by \"type\", take the sum of \"weekly_sales\", and then store as `sales_by_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9097747 0.0902253 0.       ]\n"
     ]
    }
   ],
   "source": [
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the proportion of sales at each store type by dividing by the sum of `sales_by_type`. Assign to `sales_propn_by_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    0.909775\n",
      "B    0.090225\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sales_propn_by_type = sales_by_type / sum(sales_by_type)\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group sales by \"type\" and \"is_holiday\", take the sum of `weekly_sales`, and store as `sales_by_type_is_holiday`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type  is_holiday\n",
      "A     False         2.336927e+08\n",
      "      True          2.360181e+04\n",
      "B     False         2.317678e+07\n",
      "      True          1.621410e+03\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sales_by_type_is_holiday = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the min, max, mean, and median of `weekly_sales` for each store type using `.groupby()` and `.agg().` Store this as `sales_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         min        max          mean    median\n",
      "type                                           \n",
      "A    -1098.0  293966.05  23674.667242  11943.92\n",
      "B     -798.0  232558.51  25696.678370  13336.08\n",
      "     unemployment                         fuel_price_usd_per_l            \\\n",
      "              min    max      mean median                  min       max   \n",
      "type                                                                       \n",
      "A           3.879  8.992  7.972611  8.067             0.664129  1.107410   \n",
      "B           7.170  9.765  9.279323  9.199             0.760023  1.107674   \n",
      "\n",
      "                          \n",
      "          mean    median  \n",
      "type                      \n",
      "A     0.744619  0.735455  \n",
      "B     0.805858  0.803348  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:3: FutureWarning: The provided callable <function min at 0x000001C3BC6DA520> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:3: FutureWarning: The provided callable <function max at 0x000001C3BC6DA3E0> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:3: FutureWarning: The provided callable <function mean at 0x000001C3BC6DADE0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:3: FutureWarning: The provided callable <function median at 0x000001C3BC841800> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:6: FutureWarning: The provided callable <function min at 0x000001C3BC6DA520> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:6: FutureWarning: The provided callable <function max at 0x000001C3BC6DA3E0> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:6: FutureWarning: The provided callable <function mean at 0x000001C3BC6DADE0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:6: FutureWarning: The provided callable <function median at 0x000001C3BC841800> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2625735051.py:6: FutureWarning: The provided callable <function min at 0x000001C3BC6DA520> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "print(sales_stats)\n",
    "\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Tables \n",
    "\n",
    "In pandas, pivot tables are essentially another way of performing grouped calculations. That is, the `.pivot_table()` method is an alternative to `.groupby()`.\n",
    "\n",
    "Get the mean `weekly_sales` by type using `.pivot_table()` and store as `mean_sales_by_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weekly_sales\n",
      "type              \n",
      "A     23674.667242\n",
      "B     25696.678370\n"
     ]
    }
   ],
   "source": [
    "mean_sales_by_type = sales.pivot_table(\n",
    "    values=\"weekly_sales\",\n",
    "    index=\"type\"\n",
    ")\n",
    "\n",
    "print(mean_sales_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean and median (using NumPy functions) of `weekly_sales` by `type` and store as `mean_med_sales_by_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mean       median\n",
      "      weekly_sales weekly_sales\n",
      "type                           \n",
      "A     23674.667242     11943.92\n",
      "B     25696.678370     13336.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2744108252.py:3: FutureWarning: The provided callable <function mean at 0x000001C3BC6DADE0> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  mean_med_sales_by_type = sales.pivot_table(\n",
      "C:\\Users\\joseeden\\AppData\\Local\\Temp\\ipykernel_28160\\2744108252.py:3: FutureWarning: The provided callable <function median at 0x000001C3BC841800> is currently using DataFrameGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  mean_med_sales_by_type = sales.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_med_sales_by_type = sales.pivot_table(\n",
    "    values=\"weekly_sales\",\n",
    "    index=\"type\",\n",
    "    aggfunc=[np.mean, np.median]\n",
    ")\n",
    "\n",
    "print(mean_med_sales_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean of `weekly_sales` by `type` and `is_holiday` and store as `mean_sales_by_type_holiday`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_holiday         False      True \n",
      "type                               \n",
      "A           23768.583523  590.04525\n",
      "B           25751.980533  810.70500\n"
     ]
    }
   ],
   "source": [
    "mean_sales_by_type_holiday = sales.pivot_table(\n",
    "    values=\"weekly_sales\",\n",
    "    index=\"type\",\n",
    "    columns=\"is_holiday\"\n",
    ")\n",
    "\n",
    "print(mean_sales_by_type_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the mean `weekly_sales` by `department` and `type`, filling in any missing values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department            1              2             3             4   \\\n",
      "type                                                                  \n",
      "A           30961.725379   67600.158788  17160.002955  44285.399091   \n",
      "B           44050.626667  112958.526667  30580.655000  51219.654167   \n",
      "\n",
      "department            5             6             7             8   \\\n",
      "type                                                                 \n",
      "A           34821.011364   7136.292652  38454.336818  48583.475303   \n",
      "B           63236.875000  10717.297500  52909.653333  90733.753333   \n",
      "\n",
      "department            9             10  ...            90            91  \\\n",
      "type                                    ...                               \n",
      "A           30120.449924  30930.456364  ...  85776.905909  70423.165227   \n",
      "B           66679.301667  48595.126667  ...  14780.210000  13199.602500   \n",
      "\n",
      "department             92            93            94             95  \\\n",
      "type                                                                   \n",
      "A           139722.204773  53413.633939  60081.155303  123933.787121   \n",
      "B            50859.278333   1466.274167    161.445833   77082.102500   \n",
      "\n",
      "department            96            97            98          99  \n",
      "type                                                              \n",
      "A           21367.042857  28471.266970  12875.423182  379.123659  \n",
      "B            9528.538333   5828.873333    217.428333    0.000000  \n",
      "\n",
      "[2 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "by_dept = sales.pivot_table(\n",
    "    values=\"weekly_sales\",\n",
    "    index=\"type\",\n",
    "    columns=\"department\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(by_dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the mean `weekly_sales` by `department` and `type`, filling in any missing values with 0 and summing all rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_dept = sales.pivot_table(\n",
    "    values=\"weekly_sales\",\n",
    "    index=\"type\",\n",
    "    columns=\"department\",\n",
    "    fill_value=0,\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "print(by_dept)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
